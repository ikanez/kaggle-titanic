{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Examples: 891\n",
      "Test Examples: 418\n",
      "All Examples: 1309\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script imports the Kaggle Titanic dataset in .csv form, extracts a new feature from name data, imputates missing\n",
    "values using an autoencoder initialized neural network set for regression. Lastly it builds an autoencoder initalized\n",
    "deep neural network to determine if an individual in the test set is likely to have survived. Keras is used for all NNs.\n",
    "\n",
    "Originally I randomly partitioned off 91 of 891 examples for validation testing, however, training accuracy was severely \n",
    "affected and test accuracy was very erratic depending on which 91 were chosen, i.e. insufficent data. As my primary goals \n",
    "were to practice using Pandas and neural networks, I did not look at other ways to deal with this. However, I ran the \n",
    "autoencoder on the entire dataset during training, including the test set. This obviously risks overfitting the test set,\n",
    "but I wanted to see if it would help, as that would suggest that mixing unlabeled and labeled data in such a fashion may \n",
    "be useful. \n",
    "\n",
    "There is also a massive bug- monitoring training via the verbosity setting on Keras results in Jupyter bugs. The latest\n",
    "update fixed the crashing, but now the latency is horrendous. To work around this, multiple models with increasing epochs\n",
    "are run to easily pick the best one, at the cost of a significant increase in complexity. When autoencoder initialized \n",
    "layers are used, I accidentally set it to shallow copy, and as such it is trained by every model that touches it. For some\n",
    "reason this helps by a few percent, and fixing it makes it worse, even with more epochs during the AE initialization. This\n",
    "may help counter the vanishing gradient problem in a manner reminiscent of deep belief networks, but further research on \n",
    "larger, and more varied data sets with more test runs are necessary.\n",
    "\n",
    "I feel the need to include the disclaimer that hammering everything with complex models and large deep nets\n",
    "may not always be optimal, and that this script is a little haphazard in that regard. Compared to some other public SVM \n",
    "and random forest based models on the leaderboard, this is exceptionally slow even with GPU acceleration. Time permitting,\n",
    "I want to explore combining these methods together in a committee/ensemble, as that has ranked well on the leaderboard.\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "dfTrain=pandas.read_csv('train.csv')\n",
    "dfTest=pandas.read_csv('test.csv')\n",
    "\n",
    "print('Training Examples:',len(dfTrain))\n",
    "print('Test Examples:',len(dfTest))\n",
    "dfAll=dfTrain.append(dfTest) \n",
    "print('All Examples:',len(dfAll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Determine social ranking from names to create an additional feature\n",
    "dfAll['Title']=dfAll['Name'].apply( lambda x:( (x.split(', ')[1]).split(' ')[0] ) )\n",
    "\n",
    "dfAll['Title']=dfAll['Title'].replace(to_replace=['Don.','Rev.','Master.','Dr.','Col.','Capt.',\n",
    "                                                      'Major.','Jonkheer.','Lady.','the','Sir.','Dona.'], value=1)                                          \n",
    "dfAll['Title']=dfAll['Title'].replace(to_replace=['Miss.','Mlle.','Ms.','Mrs.','Mme.','Mr.'],value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Simple imputation, based on this analysis: https://www.kaggle.com/mrisdal/titanic/exploring-survival-on-the-titanic\n",
    "dfAll['Fare']=dfAll['Fare'].replace(to_replace=[dfTest['Fare'][1044-892]],value=8.05)\n",
    "#Numpy does not allow direct comparison to np.nan. This is an admittedly ugly workaround.\n",
    "NAN=(dfTrain['Embarked'][829])\n",
    "dfAll['Embarked']=dfTrain['Embarked'].replace(to_replace=[NAN],value='C')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#one-hot encoding\n",
    "dfAll=pandas.get_dummies(dfAll, columns=['Sex'])\n",
    "dfAll=pandas.get_dummies(dfAll, columns=['Embarked'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalization\n",
    "def normalize(df,strlab):\n",
    "    df[strlab]=(df[strlab].map(lambda x: float(float(float(x) - float(numpy.min(df[strlab]))) /\n",
    "                                                     float(float(numpy.max(df[strlab])) - float(numpy.min(df[strlab]))))))\n",
    "normalize(dfAll,'Age')\n",
    "normalize(dfAll,'Parch')\n",
    "normalize(dfAll,'Pclass')\n",
    "normalize(dfAll,'Fare')\n",
    "normalize(dfAll,'SibSp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has age: 1046\n",
      "Missing age: 263\n"
     ]
    }
   ],
   "source": [
    "#Prepare to imputate age.\n",
    "dfNoAge=dfAll[numpy.isnan(dfAll['Age'])]\n",
    "dfAge=dfAll[(numpy.isnan(dfAll['Age']))==False]\n",
    "print('Has age:',len(dfAge))\n",
    "print('Missing age:',len(dfNoAge))\n",
    "\n",
    "noAgeAE=dfAll.drop(['Age','Cabin','PassengerId','Name','Survived','Ticket'],axis=1).as_matrix()\n",
    "ageTrainX=dfAge.drop(['Age','Cabin','PassengerId','Name','Survived','Ticket'],axis=1).as_matrix()\n",
    "ageTrainY=dfAge.as_matrix(columns=['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import Keras \n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ikanez/anaconda3/lib/python3.6/site-packages/keras/models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  32/1309 [..............................] - ETA: 6sTest score: 0.523910809101\n",
      "Test accuracy: 0.0412528647794\n",
      "\n",
      "20\n",
      " 992/1309 [=====================>........] - ETA: 0sTest score: 0.030650464973\n",
      "Test accuracy: 0.815126050192\n",
      "\n",
      "40\n",
      "  32/1309 [..............................] - ETA: 0sTest score: 0.0134952006523\n",
      "Test accuracy: 0.597402596947\n",
      "\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Python does not have a Multiple Imputation by Chained Equations library.\n",
    "#While I could export to R and use MICE I wanted to try neural net regression.\n",
    "#As my research frequently involves autoencoders, I used an AE to initialize the hidden before running a normal NN\n",
    "\n",
    "batch_size = 20\n",
    "nb_classes = 2\n",
    "nb_epochs = 20\n",
    "models=[]\n",
    "\n",
    "#Runs multiple models with increasing epochs, and with the option to pick an earlier one.\n",
    "#Not optimal, but helps deal with bugs and latency in jupyter notebook. Verbosity is disabled for this reason.\n",
    "for x in range(0,3):\n",
    "    nb_epoch=nb_epochs*x\n",
    "    print(nb_epoch)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(9, input_shape=(10,)))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(noAgeAE, noAgeAE,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=0, validation_data=(noAgeAE, noAgeAE))\n",
    "\n",
    "    score = model.evaluate(noAgeAE, noAgeAE, verbose=1)\n",
    "    models.append(model)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    print('')\n",
    "print('\\nDone')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AELayer=model #Selects last model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ikanez/anaconda3/lib/python3.6/site-packages/keras/models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  32/1046 [..............................] - ETA: 5sTest score: 0.492518335809\n",
      "Test accuracy: 0.000956022944551\n",
      "\n",
      "20\n",
      "  32/1046 [..............................] - ETA: 0sTest score: 0.022460946247\n",
      "Test accuracy: 0.0019120458891\n",
      "\n",
      "40\n",
      "  32/1046 [..............................] - ETA: 0sTest score: 0.0212936844128\n",
      "Test accuracy: 0.0019120458891\n",
      "\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "nb_classes = 2\n",
    "nb_epochs = 20\n",
    "models=[]\n",
    "\n",
    "#Massive bug here in which each run is not independent, and the initialized layers get backproped over on every run.\n",
    "#However, this seems to increase performance, and may be worth looking into.\n",
    "\n",
    "for x in range(0,3):\n",
    "    nb_epoch=nb_epochs*x\n",
    "    print(nb_epoch)\n",
    "    model = Sequential()\n",
    "    model.add(AELayer.layers[0])\n",
    "    model.add(AELayer.layers[1])\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(ageTrainX, ageTrainY,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=0, validation_data=(ageTrainX, ageTrainY))\n",
    "\n",
    "    score = model.evaluate(ageTrainX, ageTrainY, verbose=1)\n",
    "    models.append(model)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    print('')\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finmod=model #pick last model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imputate age from NN model\n",
    "\n",
    "dfNoAge=dfNoAge.drop(['Age'],axis=1)\n",
    "matNoAge=dfNoAge.drop(['Cabin','PassengerId','Name','Survived','Ticket'], axis=1)\n",
    "\n",
    "ages=finmod.predict(matNoAge.as_matrix(),batch_size=1)\n",
    "\n",
    "dfNoAge['Age']=ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Recombine data, and resplit to build the classifier\n",
    "dfRecombined=dfNoAge.append(dfAge)\n",
    "dfRecombined=dfRecombined.drop(['Cabin','Name','Ticket'],axis=1)\n",
    "MatAEClass=dfRecombined.drop(['Survived','PassengerId'],axis=1).as_matrix()\n",
    "\n",
    "dfRecTest=dfRecombined[numpy.isnan(dfRecombined['Survived'])]\n",
    "dfRecTest=dfRecTest.drop(['Survived'],axis=1)\n",
    "dfRecTrain=dfRecombined[numpy.isnan(dfRecombined['Survived'])==False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ikanez/anaconda3/lib/python3.6/site-packages/keras/models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  32/1309 [..............................] - ETA: 7sTest score: 0.593811725596\n",
      "Test accuracy: 0.110007639414\n",
      "\n",
      "20\n",
      "  32/1309 [..............................] - ETA: 0sTest score: 0.00279010708317\n",
      "Test accuracy: 0.437738731401\n",
      "\n",
      "40\n",
      " 768/1309 [================>.............] - ETA: 0sTest score: 0.00442516984919\n",
      "Test accuracy: 0.545454545409\n",
      "\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "I tried splitting off 91 examples for validation but training accuracy was hit pretty hard,\n",
    "and validation accuracy was extremely erratic depending on how the random sampling went.\n",
    "Since nothing is going right I decided to risk deliberately overfitting the test set\n",
    "and decided to initialize the first layer of a deep neural net by running an AE on all the data.\n",
    "This might be a viable way to use data with missing labels.\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 1\n",
    "nb_classes = 2\n",
    "nb_epochs = 20\n",
    "models=[]\n",
    "\n",
    "for x in range(0,3):\n",
    "    nb_epoch=nb_epochs*x\n",
    "    print(nb_epoch)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(7, input_shape=(11,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(11))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Verbosity disabled since it triggers a juptyer notebook bug and crashes the training\n",
    "\n",
    "    history = model.fit(MatAEClass, MatAEClass,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=0, validation_data=(MatAEClass, MatAEClass))\n",
    "\n",
    "    score = model.evaluate(MatAEClass, MatAEClass, verbose=1)\n",
    "    models.append(model)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    print('')\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AEModel=model #Pick model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reseperate training and test data; format labels\n",
    "finTrainX=dfRecTrain.drop(['Survived','PassengerId'],axis=1).as_matrix()\n",
    "dfRecTrain=pandas.get_dummies(dfRecTrain, columns=['Survived'])\n",
    "finTrainY=dfRecTrain.as_matrix(columns=['Survived_0.0','Survived_1.0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ikanez/anaconda3/lib/python3.6/site-packages/keras/models.py:848: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/891 [>.............................] - ETA: 5sTest score: 0.672483369223\n",
      "Test accuracy: 0.616161616295\n",
      "\n",
      "10\n",
      " 32/891 [>.............................] - ETA: 0sTest score: 0.413497737562\n",
      "Test accuracy: 0.826038158435\n",
      "\n",
      "20\n",
      " 32/891 [>.............................] - ETA: 0sTest score: 0.396185207447\n",
      "Test accuracy: 0.840628506827\n",
      "\n",
      "30\n",
      " 32/891 [>.............................] - ETA: 0sTest score: 0.395122745038\n",
      "Test accuracy: 0.839506173508\n",
      "\n",
      "40\n",
      " 32/891 [>.............................] - ETA: 0sTest score: 0.38853517265\n",
      "Test accuracy: 0.838383838585\n",
      "\n",
      "50\n",
      "608/891 [===================>..........] - ETA: 0sTest score: 0.383427289566\n",
      "Test accuracy: 0.838383839053\n",
      "\n",
      "60\n",
      "480/891 [===============>..............] - ETA: 0sTest score: 0.374094809697\n",
      "Test accuracy: 0.843995510194\n",
      "\n",
      "70\n",
      "480/891 [===============>..............] - ETA: 0sTest score: 0.369750533541\n",
      "Test accuracy: 0.849607183609\n",
      "\n",
      "80\n",
      "480/891 [===============>..............] - ETA: 0sTest score: 0.367399496629\n",
      "Test accuracy: 0.846240180911\n",
      "\n",
      "90\n",
      "576/891 [==================>...........] - ETA: 0sTest score: 0.365263512975\n",
      "Test accuracy: 0.847362514029\n",
      "\n",
      "100\n",
      "576/891 [==================>...........] - ETA: 0sTest score: 0.370575594849\n",
      "Test accuracy: 0.847362514698\n",
      "\n",
      "110\n",
      "512/891 [================>.............] - ETA: 0sTest score: 0.363261480842\n",
      "Test accuracy: 0.845117845118\n",
      "\n",
      "120\n",
      "448/891 [==============>...............] - ETA: 0sTest score: 0.357335923363\n",
      "Test accuracy: 0.847362514029\n",
      "\n",
      "130\n",
      " 32/891 [>.............................] - ETA: 0sTest score: 0.353726279488\n",
      "Test accuracy: 0.854096522101\n",
      "\n",
      "140\n",
      " 32/891 [>.............................] - ETA: 0sTest score: 0.347605500492\n",
      "Test accuracy: 0.857463525468\n",
      "\n",
      "150\n",
      " 32/891 [>.............................] - ETA: 0sTest score: 0.35258014654\n",
      "Test accuracy: 0.854096522101\n",
      "\n",
      "160\n",
      " 32/891 [>.............................] - ETA: 0sTest score: 0.339529952216\n",
      "Test accuracy: 0.860830528166\n",
      "\n",
      "170\n",
      " 32/891 [>.............................] - ETA: 0sTest score: 0.351072853412\n",
      "Test accuracy: 0.856341190343\n",
      "\n",
      "180\n",
      " 32/891 [>.............................] - ETA: 0sTest score: 0.347656933179\n",
      "Test accuracy: 0.861952862622\n",
      "\n",
      "190\n",
      " 32/891 [>.............................] - ETA: 0sTest score: 0.341037058887\n",
      "Test accuracy: 0.863075196409\n",
      "\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Again, theres the massive bug where the first hidden layer is shared by, and subsequently trained by, all the models.\n",
    "However this has given me the best test accuracy so far, and fixing this bug drops accuracy by a few percent at least.\n",
    "Given that deep nets tend to have vanishing gradients further up this may be a way to counter that, similar to how AE based\n",
    "deep belief networks are used.\n",
    "\n",
    "I've messed around a lot with varying layers, hidden units, activations, optimizers, loss functions, dropout \n",
    "and regularization, and while the AE initialization helped, getting further with just NN/DNN architectures will\n",
    "require either luck or experience. Other public submissions further up the leaderboard have used genetic algorithms and\n",
    "multiple models in a committee. Integrating elements of this approach may be helpful.\n",
    "\"\"\"\n",
    "batch_size = 1\n",
    "nb_classes = 2\n",
    "nb_epochs = 10 \n",
    "models=[]\n",
    "\n",
    "for x in range(0,20):\n",
    "    nb_epoch=nb_epochs*x\n",
    "    print(nb_epoch)\n",
    "    model = Sequential()\n",
    "    model.add(AEModel.layers[0])\n",
    "    model.add(AEModel.layers[1])\n",
    "    #model.add(Dense(100, input_shape=(11,)))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(Dropout(.2))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(.2))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Verbosity disabled since it triggers a juptyer notebook bug and crashes the training\n",
    "\n",
    "    history = model.fit(finTrainX, finTrainY,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=0, validation_data=(finTrainX, finTrainY))\n",
    "\n",
    "    score = model.evaluate(finTrainX, finTrainY, verbose=1)\n",
    "    models.append(model)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    print('')\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/418 [=>............................] - ETA: 0s[0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 1 1 0 0 0\n",
      " 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 0 1 1 1\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0\n",
      " 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0\n",
      " 0 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1\n",
      " 1 0 0 0 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 0 1 1\n",
      " 0 1 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 0\n",
      " 1 1 0 0 0 0 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "#Converts things to CSV for Kaggle submission\n",
    "\n",
    "model=models[-1]\n",
    "\n",
    "outs = model.predict_classes(dfRecTest.drop(['PassengerId'],axis=1).as_matrix())\n",
    "print(outs)\n",
    "\n",
    "output = []\n",
    "output.append([\"PassengerId\",\"Survived\"])\n",
    "matTestId=dfRecTest.as_matrix(columns=['PassengerId'])\n",
    "for x in range(0,len(outs)):\n",
    "    output.append([matTestId[x][0],int(outs[x])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output2 = pd.DataFrame(output, columns=['PassengerId','Survived' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PassengerId</td>\n",
       "      <td>Survived</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>914</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>921</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>925</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>928</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>931</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>933</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>939</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>946</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>950</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>957</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>968</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>975</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>976</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>977</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>980</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>983</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>985</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>994</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1008</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1013</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1019</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1038</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>1273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>1275</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>1277</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>1278</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>1279</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>1280</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1281</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>1282</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>1283</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>1284</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>1285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>1286</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>1287</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>1288</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>1289</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>1290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>1291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>1292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>1293</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>1294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>1295</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>1296</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1297</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1299</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1303</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1304</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>419 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "0    PassengerId  Survived\n",
       "1            902         0\n",
       "2            914         1\n",
       "3            921         0\n",
       "4            925         1\n",
       "5            928         1\n",
       "6            931         0\n",
       "7            933         0\n",
       "8            939         0\n",
       "9            946         0\n",
       "10           950         0\n",
       "11           957         1\n",
       "12           968         0\n",
       "13           975         0\n",
       "14           976         0\n",
       "15           977         0\n",
       "16           980         0\n",
       "17           983         0\n",
       "18           985         0\n",
       "19           994         0\n",
       "20           999         0\n",
       "21          1000         0\n",
       "22          1003         1\n",
       "23          1008         0\n",
       "24          1013         0\n",
       "25          1016         0\n",
       "26          1019         0\n",
       "27          1024         0\n",
       "28          1025         0\n",
       "29          1038         0\n",
       "..           ...       ...\n",
       "389         1273         0\n",
       "390         1275         0\n",
       "391         1277         1\n",
       "392         1278         0\n",
       "393         1279         0\n",
       "394         1280         0\n",
       "395         1281         1\n",
       "396         1282         0\n",
       "397         1283         1\n",
       "398         1284         1\n",
       "399         1285         0\n",
       "400         1286         0\n",
       "401         1287         1\n",
       "402         1288         0\n",
       "403         1289         1\n",
       "404         1290         0\n",
       "405         1291         0\n",
       "406         1292         1\n",
       "407         1293         0\n",
       "408         1294         1\n",
       "409         1295         1\n",
       "410         1296         0\n",
       "411         1297         0\n",
       "412         1298         0\n",
       "413         1299         0\n",
       "414         1301         1\n",
       "415         1303         1\n",
       "416         1304         1\n",
       "417         1306         1\n",
       "418         1307         0\n",
       "\n",
       "[419 rows x 2 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2.to_csv(path_or_buf='outputAENNRAEDNN.csv',index=False,columns=['PassengerId','Survived'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
